<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/hexo/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/hexo/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/hexo/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/hexo/images/logo.svg" color="#222">

<link rel="stylesheet" href="/hexo/css/main.css">


<link rel="stylesheet" href="/hexo/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhongweiqiang.github.io","root":"/hexo/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="入门数学上将一维数组称为向量，将二维数组称为矩阵。将一般化后的向量或矩阵统称为张量(tensor)">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门与实践">
<meta property="og:url" content="https://zhongweiqiang.github.io/hexo/2020/05/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="钟伟强的小窝">
<meta property="og:description" content="入门数学上将一维数组称为向量，将二维数组称为矩阵。将一般化后的向量或矩阵统称为张量(tensor)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103023040721.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103024840215.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103031354946.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103031457395.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103034951164.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103040345944.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103042635385.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103044811244.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103045527284.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103152342786.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103051003063.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103154630493.png">
<meta property="og:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103154917443.png">
<meta property="article:published_time" content="2020-05-12T13:51:57.000Z">
<meta property="article:modified_time" content="2020-05-12T15:58:47.770Z">
<meta property="article:author" content="钟伟强">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zhongweiqiang.github.io/hexo/images/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/image-20200103023040721.png">

<link rel="canonical" href="https://zhongweiqiang.github.io/hexo/2020/05/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习入门与实践 | 钟伟强的小窝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/hexo/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">钟伟强的小窝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/hexo/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/hexo/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/hexo/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/hexo/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhongweiqiang.github.io/hexo/2020/05/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8_%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/hexo/images/avatar.gif">
      <meta itemprop="name" content="钟伟强">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="钟伟强的小窝">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习入门与实践
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-12 21:51:57 / 修改时间：23:58:47" itemprop="dateCreated datePublished" datetime="2020-05-12T21:51:57+08:00">2020-05-12</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/hexo/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><p>数学上将一维数组称为向量，将二维数组称为矩阵。将一般化后的向量或矩阵统称为张量(tensor)<br><a id="more"></a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">B = np.array([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(A.shape)  <span class="comment"># (2, 2) 获取矩阵形状</span></span><br><span class="line">print(A.dtype)  <span class="comment"># int64 矩阵元素数据类型</span></span><br><span class="line">print(A * B)  <span class="comment"># 广播</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">[[ 2  6]</span></span><br><span class="line"><span class="string"> [ 6 12]]</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">print(A[<span class="number">0</span>][<span class="number">1</span>])  <span class="comment"># 2 访问元素</span></span><br><span class="line"></span><br><span class="line">A = A.flatten()</span><br><span class="line">print(A)  <span class="comment"># [1 2 3 4] 将多维数组转为一维数组</span></span><br><span class="line"></span><br><span class="line">print(A[np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>])])  <span class="comment"># [1 2 4]</span></span><br><span class="line">print(A &gt; <span class="number">2</span>)  <span class="comment"># [False False  True  True]</span></span><br><span class="line">print(A[A&gt;<span class="number">2</span>])  <span class="comment"># [3 4]</span></span><br></pre></td></tr></table></figure>
<h3 id="matplotlib绘图"><a href="#matplotlib绘图" class="headerlink" title="matplotlib绘图"></a>matplotlib绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">6</span>, <span class="number">0.1</span>)</span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)</span><br><span class="line">plt.plot(x, y1, label=<span class="string">'sin'</span>, linestyle=<span class="string">'dashdot'</span>)</span><br><span class="line">plt.plot(x, y2, linestyle=<span class="string">'--'</span>, label=<span class="string">'cos'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">plt.title(<span class="string">'sin &amp; cos'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103023040721.png" alt="image-20200103023040721"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取图片，不安装pillow包只能读png格式，安装则可读取任意格式</span></span><br><span class="line"><span class="keyword">from</span> matplotlib.image <span class="keyword">import</span> imread</span><br><span class="line"><span class="comment"># from PIL import Image</span></span><br><span class="line"></span><br><span class="line">img = imread(<span class="string">'00003.jpg'</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h3 id="感知机基础"><a href="#感知机基础" class="headerlink" title="感知机基础"></a>感知机基础</h3><p>感知机接收多个信号，输出一个信号。</p>
<p>下图为一个接收两个输入信号的感知机。$x_1, x_2$为输入信号，$y$是输出信号，$w_1, w_2$为权重，$\bigcirc$称为神经元或节点，输入信号被送往神经元时，会被乘以固定的权重($w_1x_1, w_2x_2$)，神经元会计算传送过来的信号的总和，当总和超过某个界限值时，才输出1(未激活输出0)，称为神经元被激活，这里的界限值称为阈值，用$\theta$表示</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103024840215.png" alt="image-20200103024840215"></p>
<p>感知机的数学表示：</p>
<script type="math/tex; mode=display">
y = 
\left\{
    \matrix{
        0 \ \ \ \ (w_1x_1 + w_2x_2 \leq \theta) \\
        1 \ \ \  \ (w_1x_1 + w_2x_2 \gt \theta)
    }
\right.</script><p>与门：</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103031354946.png" alt="image-20200103031354946"></p>
<p>非门(都为true则为false，其他都是true)</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103031457395.png" alt="image-20200103031457395"></p>
<p>满足感知机的条件：</p>
<p>$(w_1, w_2, \theta) = (0.5, 0.3, 0.7)$时，满足条件</p>
<p>学习是确定合适参数的过程，人们要做的是思考感知机的构造(模型)，并把训练数据交给计算机</p>
<h3 id="感知机实现"><a href="#感知机实现" class="headerlink" title="感知机实现"></a>感知机实现</h3><h4 id="与门"><a href="#与门" class="headerlink" title="与门"></a>与门</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AND</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    w1, w2, theta = <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.7</span></span><br><span class="line">    tmp = x1 * w1 + x2*w2</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= theta:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">elif</span> tmp &gt; theta:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(AND(<span class="number">0</span>, <span class="number">0</span>))  <span class="comment"># 0</span></span><br><span class="line">print(AND(<span class="number">1</span>, <span class="number">0</span>))  <span class="comment"># 0</span></span><br><span class="line">print(AND(<span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># 0</span></span><br><span class="line">print(AND(<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 0</span></span><br></pre></td></tr></table></figure>
<h4 id="权重与偏置"><a href="#权重与偏置" class="headerlink" title="权重与偏置"></a>权重与偏置</h4><p><strong>与门：</strong></p>
<p>将公式中的$\theta$换成$-b$：</p>
<script type="math/tex; mode=display">
y = 
\left\{
    \matrix{
        0 \ \ \ \ (b + w_1x_1 + w_2x_2 \leq 0) \\
        1 \ \ \  \ (b + w_1x_1 + w_2x_2 \gt 0)
    }
\right.</script><p>式中，$b$称为偏置，$w_1和w_2$称为权重，感知机通过计算输入信号和权重的乘积，然后加上偏置，如果值大于0则输出1，否则输出0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">w = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">b = <span class="number">-0.7</span></span><br><span class="line">print(w * x)  <span class="comment"># [0.  0.5]</span></span><br><span class="line">print(np.sum(w * x))  <span class="comment"># 0.5</span></span><br><span class="line">print(np.sum(w * x) + b)  <span class="comment"># -0.19999999999999996</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AND</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    x = np.array([x1, x2])</span><br><span class="line">    w = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">    b = <span class="number">-0.7</span></span><br><span class="line">    tmp = np.sum(w * x) + b</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">NAND</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    x = np.array([x1, x2])</span><br><span class="line">    w = np.array([<span class="number">-0.5</span>, <span class="number">-0.5</span>])</span><br><span class="line">    b = <span class="number">0.7</span></span><br><span class="line">    tmp = np.sum(w * x) + b</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OR</span><span class="params">(x1, x2)</span>:</span></span><br><span class="line">    x = np.array([x1, x2])</span><br><span class="line">    w = np.array([<span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">    b = <span class="number">-0.2</span></span><br><span class="line">    tmp = np.sum(w * x) + b</span><br><span class="line">    <span class="keyword">if</span> tmp &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>这里将$-\theta$命名为偏置$b$，偏置和权重$w_1, w_2$的作用不同，$w_1$和$w_2$是控制输入信号重要性的参数，偏置是调整神经元被激活的容易程度的参数，偏置的值决定了神经元被激活的难易程度。</p>
<p>或门、非门仅权重和偏置与与门不同</p>
<h4 id="线性与非线性"><a href="#线性与非线性" class="headerlink" title="线性与非线性"></a>线性与非线性</h4><p>(单层)感知机只能分割直线空间，非线性空间无法被分割</p>
<p>异或门可以采用多层感知机实现，异或门是一种多层结构的神经网络，开始一层为第0层，然后…</p>
<p>叠加了多层的感知机也称为多层感知机(multi-layer perceptron)</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103034951164.png" alt="image-20200103034951164"></p>
<p>上图感知机总共由3层构成，因为拥有权重的层只有2层(0层到1层之间，1层到2层之间)，所以称为2层感知机，也有文献称为3层感知机</p>
<ul>
<li>第0层的两个神经元接收输入信号，并将信号发送至第1层的神经元</li>
<li>第1层的神经元将信号发送至第2层的神经元，第2层的神经元输出$y$</li>
</ul>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><ul>
<li>感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值</li>
<li>感知机将权重和偏置设定为参数</li>
<li>使用感知机可以表示与门和或门等逻辑电路</li>
<li>异或门无法通过单层感知机来表示</li>
<li>使用2层感知机可以表示异或门</li>
<li>单层感知机只能表示线性空间，多层感知机可以表示非线性空间</li>
</ul>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络的重要性质是可以自动的从数据中学习到合适的权重参数</p>
<h3 id="从感知机到神经网络"><a href="#从感知机到神经网络" class="headerlink" title="从感知机到神经网络"></a>从感知机到神经网络</h3><h4 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h4><p>神经网络表示如下图：</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103040345944.png" alt="image-20200103040345944"></p>
<p>最左边一列称为输入层，最右边一列称为输出层，中间一列称为中间层。中间层也称为隐藏层(隐藏的神经元肉眼不可见)。该网络一共由3层神经元构成，实质是只有2层神经元有权重，因此称为2层网络。</p>
<h4 id="感知机-1"><a href="#感知机-1" class="headerlink" title="感知机"></a>感知机</h4><script type="math/tex; mode=display">
y = 
\left\{
    \matrix{
        0 \ \ \ \ (b + w_1x_1 + w_2x_2 \leq 0) \\
        1 \ \ \  \ (b + w_1x_1 + w_2x_2 \gt 0)
    } \tag{3.1}
\right.</script><p>$b$为偏置，用于控制神经元被激活的难易程度，$w_1和w_2$表示各个信号的权重参数，用于控制各个信号的重要性</p>
<p>输入信号与各自的权重相乘后加上偏置，可用下式表示：</p>
<script type="math/tex; mode=display">
y = h(b + w_1x_1 + w_2x_2) \tag{3.2}</script><script type="math/tex; mode=display">
h(x) = 
\left\{
    \matrix{
        0 \ \ \ (x \leq 0) \\ 
        1 \ \ \ (x \gt 0)
    } \tag{3.3}
\right.</script><p>函数$h(x)$，输出超过0时返回1，否则返回0</p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>函数$h(x)$将输入信号的总和转换为输出信号，这种函数一般称为激活函数(activation function)。</p>
<script type="math/tex; mode=display">
a = b + w_1x_1 + w_2x_2  \tag{3.4}</script><script type="math/tex; mode=display">
y = h(a) \tag{3.5}</script><p>首先计算加权输入信号和偏置的总和，记为a，然后用$h()$函数将a转换为输出y</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103042635385.png" alt="image-20200103042635385"></p>
<p>上图中，表示神经元的$\bigcirc$中显示了激活函数的计算过程，即信号的加权总和为节点a，然后节点a被激活函数$h()$转换为节点$y$</p>
<p>激活函数是连接感知机和神经网络的桥梁</p>
<h3 id="激活函数-1"><a href="#激活函数-1" class="headerlink" title="激活函数"></a>激活函数</h3><p>式3.3表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出，这种函数称为阶跃函数，即感知机中使用阶跃函数作为激活函数。在众多候选函数中，感知机使用了阶跃函数作为激活函数</p>
<h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><p>神经网络中经常是用的一个激活函数是sigmoid函数，表示如下：</p>
<script type="math/tex; mode=display">
h(x) = \frac{1}{1 + e^{-x}} \tag{3.6}</script><p>激活函数是在给定某个输入后，会返回某个输出的转换器。</p>
<p>神经网络中使用sigmoid函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。</p>
<h4 id="阶跃函数的实现"><a href="#阶跃函数的实现" class="headerlink" title="阶跃函数的实现"></a>阶跃函数的实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># 只能接收实数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x &gt; <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> y.astype(np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">-1.</span>, <span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line">y = x &gt; <span class="number">0</span></span><br><span class="line">print(y)  <span class="comment"># [False  True  True]</span></span><br><span class="line">print(y.astype(np.int))  <span class="comment"># [0 1 1]</span></span><br><span class="line"><span class="comment"># astype转换数组的类型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">step_function</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array(x &gt; <span class="number">0</span>, dtype=np.int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-5.</span>, <span class="number">5.</span>, <span class="number">0.1</span>)</span><br><span class="line">y = step_function(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(<span class="number">-0.1</span>, <span class="number">1.1</span>)  <span class="comment"># 指定y周范围</span></span><br><span class="line">plt.savefig(<span class="string">'step_function.png'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103044811244.png" alt="image-20200103044811244"></p>
<p>阶跃函数以0为界，输出从0切换为1，值呈阶梯式变化，所以称为阶跃函数</p>
<h4 id="sigmoid函数的实现"><a href="#sigmoid函数的实现" class="headerlink" title="sigmoid函数的实现"></a>sigmoid函数的实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>)</span><br><span class="line">y = sigmoid(x)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.ylim(<span class="number">-0.1</span>, <span class="number">1.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103045527284.png" alt="image-20200103045527284"></p>
<p>sigmoid函数是一条平滑曲线，输出随着输入发生连续性变化，而阶跃函数以0为界，输出发生急剧性变化。</p>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103152342786.png" alt="image-20200103152342786"></p>
<ul>
<li><p>感知机中神经元之间的流动为0或1的二元信号，神经网络中流动的是连续的实数信号</p>
</li>
<li><p>阶跃函数和sigmoid函数的输出都在0～1之间</p>
</li>
<li><p>两者都是非线性函数</p>
</li>
</ul>
<h4 id="ReLU-Rectified-Linear-Unit-函数"><a href="#ReLU-Rectified-Linear-Unit-函数" class="headerlink" title="ReLU(Rectified Linear Unit)函数"></a>ReLU(Rectified Linear Unit)函数</h4><p>ReLU函数在输入大于0时，直接输出该值，在输入小于0时，输入0</p>
<p>公式如下：</p>
<script type="math/tex; mode=display">
h(x) = 
    \left\{
        \matrix{
            x \ \ \ (x \gt 0) \\
            0 \ \ \ (x \leq 0)
        } \tag{3.7}
    \right.</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)  <span class="comment"># 输入较大的值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-5.0</span>, <span class="number">5.0</span>, <span class="number">0.1</span>)</span><br><span class="line">y = relu(x)</span><br><span class="line">plt.plot(x, y, linestyle=<span class="string">'dashed'</span>)</span><br><span class="line">plt.ylim(<span class="number">-0.1</span>, <span class="number">1.1</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">plt.title(<span class="string">'sigmoid'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103051003063.png" alt="image-20200103051003063"></p>
<h3 id="多维数组计算"><a href="#多维数组计算" class="headerlink" title="多维数组计算"></a>多维数组计算</h3><h4 id="多维数组"><a href="#多维数组" class="headerlink" title="多维数组"></a>多维数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>,<span class="number">4</span>, <span class="number">3</span>]])</span><br><span class="line">print(A.shape)  <span class="comment"># (2, 3)</span></span><br><span class="line">print(np.ndim(A))  <span class="comment"># 2 数组维数</span></span><br></pre></td></tr></table></figure>
<h4 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">B = np.array([[<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line">print(np.dot(A, B))  <span class="comment"># 点积</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[19 22]</span></span><br><span class="line"><span class="string"> [43 50]]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  <span class="comment"># 2 x 3</span></span><br><span class="line">B = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>]])  <span class="comment"># 3 x 2</span></span><br><span class="line">print(np.dot(A, B))  <span class="comment"># 2 x 2</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[22 28]</span></span><br><span class="line"><span class="string"> [49 64]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>矩阵点积：</p>
<script type="math/tex; mode=display">
(a \times b) \cdot (b \times c) = a \times c</script><h4 id="神经网络的内积"><a href="#神经网络的内积" class="headerlink" title="神经网络的内积"></a>神经网络的内积</h4><p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103154630493.png" alt="image-20200103154630493"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">W = np.array([[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]])</span><br><span class="line">print(np.dot(X, W))  <span class="comment"># [ 5 11 17]</span></span><br></pre></td></tr></table></figure>
<h3 id="3层神经网络的实现"><a href="#3层神经网络的实现" class="headerlink" title="3层神经网络的实现"></a>3层神经网络的实现</h3><p><img src="/hexo/images/深度学习入门_基于python的理论与实践/image-20200103154917443.png" alt="image-20200103154917443"></p>
<p>3层神经网络：输入层(第0层)有2个神经元，第1个隐藏层(第1层)有3个神经元，第2个隐藏层(第2层)有2个神经元，输出层(第3层)有2个神经元</p>
<h3 id="输出层的设计"><a href="#输出层的设计" class="headerlink" title="输出层的设计"></a>输出层的设计</h3><p>神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用<code>softmax</code>函数。</p>
<p>分类问题是用的<code>softmax</code>函数如下表示：</p>
<script type="math/tex; mode=display">
y_k = \frac{e^{a_k}}{\sum_{i=1}^{n}e^{a_i}}</script><p>假设输出层共有n个神经元，计算第$k$个神经元的输出$y_k$。<code>softmax</code>函数的分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和</p>
<p><code>softmax</code>函数的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">0.3</span>, <span class="number">2.9</span>, <span class="number">4.0</span>])</span><br><span class="line">exp_a = np.exp(a)</span><br><span class="line">print(exp_a)  <span class="comment"># [ 1.34985881 18.17414537 54.59815003]</span></span><br><span class="line">sum_exp_a = np.sum(exp_a)</span><br><span class="line">print(sum_exp_a)  <span class="comment"># 74.1221542101633</span></span><br><span class="line">y = exp_a / sum_exp_a</span><br><span class="line">print(y)  <span class="comment"># [0.01821127 0.24519181 0.73659691]</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(a)</span>:</span></span><br><span class="line">    exp_a = np.exp(a)</span><br><span class="line">    sum_exp_a = np.sum(exp_a)</span><br><span class="line">    y = exp_a / sum_exp_a</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure>
<h4 id="输出层的神经元数量"><a href="#输出层的神经元数量" class="headerlink" title="输出层的神经元数量"></a>输出层的神经元数量</h4><p>输出层的神经元数量需要根据待解决问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量</p>
<h3 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h3><ul>
<li>神经网络中的激活函数使用平滑变化的sigmoid函数或<code>ReLU</code>函数。</li>
<li>通过巧妙地使用<code>NumPy</code>多维数组，可以高效地实现神经网络。</li>
<li>机器学习的问题大体上可以分为回归问题和分类问题。</li>
<li>关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用<code>softmax</code>函数。</li>
<li>分类问题中，输出层的神经元的数量设置为要分类的类别数。</li>
<li>输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。</li>
</ul>
<h2 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h2><p>学习的过程是从训练数据中自动获取最优权重的过程</p>
<p>神经网络能够学习，需要损失函数指标。学习的目的就是以该损失函数为基准，找出能使他的值达到最小的权重参数。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所使用的指标称为损失函数(loss function)。损失函数可以使用任意函数，但一般使用均方误差和交叉熵误差等</p>
<h4 id="均方误差-mean-squared-error"><a href="#均方误差-mean-squared-error" class="headerlink" title="均方误差(mean squared error)"></a>均方误差(mean squared error)</h4><p>公式表示如下：</p>
<script type="math/tex; mode=display">
E = \frac{1}{2}\sum _k{(y_k - t_k)^2}{}</script><p>式中：$y_k$表示神经网络的输出，$t_k$表示监督数据，$k$表示数据的维数</p>
<h4 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h4><script type="math/tex; mode=display">
E = -\sum_k{t_k\text{log}y_k}</script><p>式中，$y_k$表示神经网络的输出，$t_k$是正确解标签。</p>
<h4 id="mini-batch学习"><a href="#mini-batch学习" class="headerlink" title="mini-batch学习"></a>mini-batch学习</h4><p>使用训练数据进行学习，严格来说，是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。如果训练数据有100个，那么就要把100个损失函数的总和作为学习的指标</p>
<p>交叉熵计算训练数据的损失函数总和：</p>
<script type="math/tex; mode=display">
E = -\frac{1}{N}\sum_n\sum_k{t_{nk}\text{log}y_{nk}}</script><p>式中，假设数据有$N$个，$t<em>{nk}$表示第$n$个数据的第$k$个元素的值($y</em>{nk}$是神经网络的输出，$t_{nk}$是监督数据)。</p>
<h3 id="学习算法的实现"><a href="#学习算法的实现" class="headerlink" title="学习算法的实现"></a>学习算法的实现</h3><p>神经网络的学习步骤：</p>
<ul>
<li>mini-batch<ul>
<li>从训练数据中随机选出一部分数据，这部分数据称为mini-batch。目标是减小mini-batch的损失函数的值</li>
</ul>
</li>
<li>计算梯度<ul>
<li>为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向</li>
</ul>
</li>
<li>更新参数<ul>
<li>将权重参数沿梯度方向进行微小更新</li>
</ul>
</li>
<li>重复<ul>
<li>重复步骤1、步骤2、步骤3</li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          

            -->
          <div class="post-tags">
              <a class="a_tag" href="/hexo/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"> 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/hexo/2020/05/12/%E6%B5%8B%E8%AF%95github%E6%9B%B4%E6%96%B0%E5%90%8C%E6%AD%A5/" rel="prev" title="测试github更新同步">
      <i class="fa fa-chevron-left"></i> 测试github更新同步
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#入门"><span class="nav-number">1.</span> <span class="nav-text">入门</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#matplotlib绘图"><span class="nav-number">1.1.</span> <span class="nav-text">matplotlib绘图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#感知机"><span class="nav-number">2.</span> <span class="nav-text">感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机基础"><span class="nav-number">2.1.</span> <span class="nav-text">感知机基础</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机实现"><span class="nav-number">2.2.</span> <span class="nav-text">感知机实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#与门"><span class="nav-number">2.2.1.</span> <span class="nav-text">与门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#权重与偏置"><span class="nav-number">2.2.2.</span> <span class="nav-text">权重与偏置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#线性与非线性"><span class="nav-number">2.2.3.</span> <span class="nav-text">线性与非线性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#summary"><span class="nav-number">2.2.4.</span> <span class="nav-text">summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络"><span class="nav-number">3.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#从感知机到神经网络"><span class="nav-number">3.1.</span> <span class="nav-text">从感知机到神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络-1"><span class="nav-number">3.1.1.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#感知机-1"><span class="nav-number">3.1.2.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#激活函数"><span class="nav-number">3.1.3.</span> <span class="nav-text">激活函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数-1"><span class="nav-number">3.2.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid函数"><span class="nav-number">3.2.1.</span> <span class="nav-text">sigmoid函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#阶跃函数的实现"><span class="nav-number">3.2.2.</span> <span class="nav-text">阶跃函数的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sigmoid函数的实现"><span class="nav-number">3.2.3.</span> <span class="nav-text">sigmoid函数的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU-Rectified-Linear-Unit-函数"><span class="nav-number">3.2.4.</span> <span class="nav-text">ReLU(Rectified Linear Unit)函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多维数组计算"><span class="nav-number">3.3.</span> <span class="nav-text">多维数组计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#多维数组"><span class="nav-number">3.3.1.</span> <span class="nav-text">多维数组</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵乘法"><span class="nav-number">3.3.2.</span> <span class="nav-text">矩阵乘法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络的内积"><span class="nav-number">3.3.3.</span> <span class="nav-text">神经网络的内积</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3层神经网络的实现"><span class="nav-number">3.4.</span> <span class="nav-text">3层神经网络的实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#输出层的设计"><span class="nav-number">3.5.</span> <span class="nav-text">输出层的设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输出层的神经元数量"><span class="nav-number">3.5.1.</span> <span class="nav-text">输出层的神经元数量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-1"><span class="nav-number">3.6.</span> <span class="nav-text">summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络的学习"><span class="nav-number">4.</span> <span class="nav-text">神经网络的学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">4.1.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#均方误差-mean-squared-error"><span class="nav-number">4.1.1.</span> <span class="nav-text">均方误差(mean squared error)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵误差"><span class="nav-number">4.1.2.</span> <span class="nav-text">交叉熵误差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mini-batch学习"><span class="nav-number">4.1.3.</span> <span class="nav-text">mini-batch学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习算法的实现"><span class="nav-number">4.2.</span> <span class="nav-text">学习算法的实现</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="钟伟强"
      src="/hexo/images/avatar.gif">
  <p class="site-author-name" itemprop="name">钟伟强</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/hexo/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/hexo/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/hexo/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhongweiqiang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhongweiqiang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:meixueyinhu@gmail.com" title="E-Mail → mailto:meixueyinhu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/2398343741" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2398343741" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://plus.google.com/meixueyinhu" title="Google → https:&#x2F;&#x2F;plus.google.com&#x2F;meixueyinhu" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>Google</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://114.67.95.174:9011/" title="http:&#x2F;&#x2F;114.67.95.174:9011" rel="noopener" target="_blank">我的网站</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">钟伟强</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/hexo/lib/anime.min.js"></script>
  <script src="/hexo/lib/velocity/velocity.min.js"></script>
  <script src="/hexo/lib/velocity/velocity.ui.min.js"></script>

<script src="/hexo/js/utils.js"></script>

<script src="/hexo/js/motion.js"></script>


<script src="/hexo/js/schemes/pisces.js"></script>


<script src="/hexo/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
